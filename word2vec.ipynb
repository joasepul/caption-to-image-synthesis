{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_df = pd.read_csv('./coco-captions-2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f617c85c6484>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test.values[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_list = annot_df.values\n",
    "data = [] \n",
    "\n",
    "# iterate through each caption \n",
    "for row in annot_list: \n",
    "#     temp = [] \n",
    "      \n",
    "    # tokenize the captions into words \n",
    "#     for j in word_tokenize(row[1]): \n",
    "#         temp.append(j.lower()) \n",
    "  \n",
    "    data.append(row[1]) \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{50: 5075, 49: 5129, 67: 1110, 43: 4370, 47: 5127, 46: 4963, 66: 1302, 54: 4121, 37: 1555, 83: 163, 45: 4885, 41: 3452, 61: 2360, 62: 1961, 59: 2670, 38: 2045, 63: 1814, 42: 3944, 44: 4632, 57: 3330, 65: 1440, 48: 5298, 51: 5050, 29: 41, 52: 4605, 40: 2961, 56: 3673, 58: 3036, 60: 2421, 69: 849, 36: 1220, 53: 4537, 73: 561, 55: 3986, 31: 128, 64: 1589, 39: 2478, 77: 362, 68: 953, 76: 400, 75: 429, 74: 497, 70: 720, 72: 615, 34: 694, 30: 100, 81: 198, 96: 54, 113: 22, 35: 909, 82: 192, 98: 59, 71: 643, 86: 132, 97: 47, 92: 75, 85: 160, 84: 163, 106: 26, 87: 139, 90: 94, 99: 53, 118: 14, 91: 100, 79: 251, 122: 11, 128: 4, 94: 71, 33: 450, 101: 36, 80: 240, 78: 291, 103: 35, 102: 42, 172: 1, 93: 72, 88: 104, 32: 250, 89: 92, 111: 15, 95: 56, 110: 24, 114: 17, 148: 2, 100: 46, 105: 34, 130: 9, 107: 32, 136: 6, 124: 10, 104: 23, 109: 32, 115: 14, 200: 1, 120: 10, 123: 9, 108: 19, 173: 1, 129: 7, 116: 12, 112: 21, 28: 22, 126: 8, 156: 2, 27: 9, 132: 7, 217: 1, 154: 2, 117: 9, 125: 8, 121: 8, 137: 5, 146: 4, 145: 1, 176: 1, 144: 6, 127: 4, 141: 3, 164: 3, 131: 3, 133: 8, 140: 6, 178: 1, 249: 1, 119: 11, 139: 4, 222: 2, 134: 2, 147: 3, 170: 1, 235: 1, 159: 1, 229: 1, 151: 6, 160: 1, 165: 1, 138: 2, 226: 1, 157: 1, 203: 1, 194: 1, 168: 1, 174: 1, 179: 2, 198: 1, 241: 1, 213: 1, 216: 1, 188: 1, 25: 2, 26: 2, 24: 1, 191: 1, 161: 2, 143: 3, 202: 2, 158: 1, 21: 1, 225: 1, 152: 2, 199: 1, 190: 1, 230: 1, 171: 1, 135: 2, 169: 1, 196: 1, 205: 1, 242: 1, 243: 1, 181: 1, 244: 1, 155: 1, 233: 1, 162: 1, 246: 1}\n"
     ]
    }
   ],
   "source": [
    "hist = {}\n",
    "\n",
    "for point in data:\n",
    "    if len(point) not in hist:\n",
    "        hist[len(point)] = 1\n",
    "    else:\n",
    "        hist[len(point)] += 1\n",
    "        \n",
    "print (hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(list(hist.keys()), hist.values(), color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_cleaned = []\n",
    "for row in data:\n",
    "    if len(' '.split(row)) > 20:\n",
    "        continue\n",
    "    data_cleaned.append(row.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_tags = []\n",
    "# for i in range(len(data_cleaned)):\n",
    "#     doc_tags.append(i)\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "docs = [TaggedDocument(words=word_tokenize(_d), tags=[str(i)]) for i, _d in enumerate(data_cleaned)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words model\n",
    "# model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                             # size = 100, window = 5) \n",
    "# Skip Gram model \n",
    "# model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "                                # window = 5, sg = 1) \n",
    "\n",
    "# model_doc_100 = gensim.models.Doc2Vec(docs, vector_size=100, workers=8, dm=1, max_vocab_size=None, dm_mean=0)\n",
    "model_doc_sum = gensim.models.Doc2Vec(docs, vector_size=256, workers=8, dm=1, max_vocab_size=None, dm_mean=0, min_count=10)\n",
    "# model_doc_500 = gensim.models.Doc2Vec(docs, vector_size=500, workers=8, dm=1, max_vocab_size=None, dm_mean=0)\n",
    "# model_doc_mean = gensim.models.Doc2Vec(docs, vector_size=256, workers=8, dm=1, max_vocab_size=None, dm_mean=1, min_count=10)\n",
    "\n",
    "# dm_mean=1 enables vector averaging versus vector summing. Further testing required.\n",
    "# model_doc = gensim.models.Doc2Vec(docs, vector_size=100, min_count=5, workers=8, dm=1, max_vocab_size=None, dm_mean=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_doc_sum.save('textencodingmodel.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A woman uses a paintbrush on a melting candle.\n",
      "\n",
      "\n",
      "Mean\n",
      "\n",
      "a baseball player holding a bat over his shoulder.\n",
      "0.6386563181877136\n",
      "a bag that is filled with pens and scissors\n",
      "0.634752094745636\n",
      "a small airplane flying above a snow covered mountain.\n",
      "0.6334839463233948\n",
      "a narrow kitchen with a refrigerator at the end of it.\n",
      "0.6208945512771606\n",
      "a bus with a bike attached to the front is traveling down a street.\n",
      "0.6205453276634216\n",
      "\n",
      "Sum\n",
      "\n",
      "toothbrushes are arranged in a cup on the bathroom counter.\n",
      "0.6886410713195801\n",
      "a group of birds standing around in a field. \n",
      "0.6870393753051758\n",
      "workers cleaning a large green and white boat\n",
      "0.6720724105834961\n",
      "a little girl in a pink jacket riding a horse on the beach.\n",
      "0.6626769304275513\n",
      "a pile of assorted vegetables sitting side by side.\n",
      "0.6622759103775024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('35665', 0.6886410713195801),\n",
       " ('58802', 0.6870393753051758),\n",
       " ('70641', 0.6720724105834961),\n",
       " ('75843', 0.6626769304275513),\n",
       " ('81955', 0.6622759103775024)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1= 'A woman uses a paintbrush on a melting candle.'\n",
    "teststr = word_tokenize(s1)\n",
    "\n",
    "\n",
    "teststr = [x.lower() for x in teststr]\n",
    "# infer_vector = model_doc.infer_vector(teststr, steps=20000, alpha=0.025)\n",
    "# similar_documents = model_doc.docvecs.most_similar([infer_vector], topn = 10)\n",
    "# model_doc.docvecs.similarity_unseen_docs(model_doc, \\\n",
    "#                                          ['a', 'truck', 'loaded', 'down', 'with', 'bags', 'of', 'cans', 'in', 'the', 'bed', 'of', 'the', 'truck'], \\\n",
    "#                                          ['a', 'truck', 'bed', 'loaded', 'down'], \\\n",
    "#                                          steps=100000)\n",
    "\n",
    "# model_doc.docvecs.similarity_unseen_docs(model_doc, \\\n",
    "#                                          ['fresh', 'apple', 'juice'], \\\n",
    "#                                          ['gross', 'apple', 'juice'], \\\n",
    "#                                          steps=10000)\n",
    "\n",
    "num_outputs = 5\n",
    "\n",
    "print('\\n'+s1+'\\n')\n",
    "print('\\nMean\\n')\n",
    "inf_vec = model_doc_mean.infer_vector(teststr, steps=10000)#, alpha=1, min_alpha=0.002)\n",
    "output = model_doc_mean.docvecs.most_similar([inf_vec], topn=num_outputs)\n",
    "# for doc in similar_documents:\n",
    "#     print(data_cleaned[int(doc[0])])\n",
    "\n",
    "for i in range(num_outputs):    \n",
    "    print (data_cleaned[(int(output[i][0]) + 1)])\n",
    "    print (output[i][1])\n",
    "    \n",
    "print('\\nSum\\n')\n",
    "inf_vec = model_doc_sum.infer_vector(teststr, steps=10000)#, alpha=1, min_alpha=0.002)\n",
    "output = model_doc_sum.docvecs.most_similar([inf_vec], topn=num_outputs)\n",
    "\n",
    "for i in range(num_outputs):    \n",
    "    print (data_cleaned[(int(output[i][0]) + 1)])\n",
    "    print (output[i][1])\n",
    "output    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9549301027985857"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_doc.docvecs.similarity_unseen_docs(model_doc, \\\n",
    "                                         ['fresh', 'apple', 'juice'], \\\n",
    "                                         ['fresh', 'apple', 'juice'], \\\n",
    "                                         steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('begin', 0.81293785572052),\n",
       " ('tag', 0.7691446542739868),\n",
       " ('runner', 0.7600712180137634),\n",
       " ('read', 0.7600324153900146),\n",
       " ('student', 0.7563343644142151),\n",
       " ('own', 0.7557961940765381),\n",
       " ('balance', 0.7526955604553223),\n",
       " ('she', 0.7499886155128479),\n",
       " ('listening', 0.7498931884765625),\n",
       " ('see', 0.7497900724411011)]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_doc.most_similar(['wiimote'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
