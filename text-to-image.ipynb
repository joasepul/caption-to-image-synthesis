{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import ast\n",
    "import glob\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Concatenate, Dense, Dropout, Conv2D, Add, Dot, Lambda, Conv2DTranspose, Dot, Activation, Reshape, BatchNormalization, UpSampling2D, AveragePooling2D, GlobalAveragePooling2D, Multiply, Softmax, LeakyReLU, Flatten, MaxPool2D, MaxPool3D, Embedding, GRU\n",
    "from tensorflow.keras.layers import Convolution2D, UpSampling2D, SeparableConv2D, UpSampling3D\n",
    "from tensorflow.keras.layers import PReLU, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available:  False\n"
     ]
    }
   ],
   "source": [
    "print('GPU is available: ', tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('data_percentage', type=float)\n",
    "args = parser.parse_args()\n",
    "data_percentage = args.data_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '/coco2017/' for use on Nautilus\n",
    "# './'for local use\n",
    "path_prefix = './' \n",
    "data_percentage = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all captions\n",
    "captions = pd.read_csv(path_prefix + 'resources/coco-captions-with-categories.csv')\n",
    "sample_count = round(data_percentage * len(captions.values))\n",
    "annot_list = captions.values[:sample_count]\n",
    "data = [] \n",
    "corpus_dict = {}\n",
    "# iterate through each caption \n",
    "max_sequence_length = float(\"-inf\")\n",
    "for row in annot_list: \n",
    "      \n",
    "    captions = row[1].split('|')[:5]\n",
    "    \n",
    "    for caption in captions:\n",
    "        temp = []\n",
    "        #tokenize the captions into words \n",
    "        for j in word_tokenize(caption): \n",
    "            temp.append(j.lower()) \n",
    "            corpus_dict[j.lower()] = 0\n",
    "        if len(temp) > max_sequence_length:\n",
    "            max_sequence_length = len(temp)\n",
    "        data.append(temp) \n",
    "\n",
    "for index, word in enumerate(corpus_dict.keys()):\n",
    "    corpus_dict[word] = index + 1\n",
    "    \n",
    "corpus_size = len(corpus_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load external text embedding model (word2vec)\n",
    "w2v_model = Word2Vec.load(path_prefix + 'resources/text_encoding_full.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# encode all our captions\n",
    "captions_X = []\n",
    "caption_strings = []\n",
    "\n",
    "for row in annot_list[:sample_count]:\n",
    "    captions = row[1].split('|')[:1]\n",
    "    captions_list = []\n",
    "    captions_X_list = []\n",
    "    for caption in captions:\n",
    "        caption_conv = []\n",
    "#         print(caption)\n",
    "        for word in word_tokenize(caption.lower()):\n",
    "            caption_conv.append(w2v_model[word])\n",
    "        captions_X_list.append(np.array(caption_conv))\n",
    "        captions_list.append(caption.lower())\n",
    "    captions_X.append(captions_X_list)\n",
    "    caption_strings.append(captions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad with 0-vectors to max_sequence_length\n",
    "for i in range(len(captions_X)):\n",
    "    for j in range(len(captions_X[i])):\n",
    "        captions_X[i][j].resize((max_sequence_length, 100), refcheck=False)\n",
    "captions_X = np.array(captions_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to create the random distribution at training time\n",
    "dist_mean = np.mean(captions_X)\n",
    "dist_std = np.std(captions_X)\n",
    "print(dist_mean, dist_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_X = np.reshape(captions_X, (sample_count, max_sequence_length, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_loader(file_path):\n",
    "    '''\n",
    "    Loads and normalizes all image files in the directory file_path.\n",
    "    Returns np.array of normalized image matrices.\n",
    "    '''\n",
    "    img_paths = glob.glob(file_path + '/*')\n",
    "    data = []\n",
    "    for img_path in img_paths:\n",
    "        data.append(cv2.imread(img_path) / 255.0)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads in data and subsets it to the first 1600 samples\n",
    "# img_paths = glob.glob('./cleaned-data/00000000[0-9]*.jpg')\n",
    "# training_imgs_num = len(img_paths)\n",
    "# print(f'loaded a total of {training_imgs_num} imgs')\n",
    "imgs_all = data_loader(path_prefix + 'images/training/') # used to be path_prefix + 'cleaned-data/'\n",
    "imgs_y = imgs_all[:sample_count]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(caption_strings[0])\n",
    "# plt.imshow(imgs_y[0])\n",
    "print(len(caption_strings))\n",
    "print(len(captions_X))\n",
    "print(len(imgs_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom metric to avoid binary accuracy rounding\n",
    "import keras.metrics\n",
    "def image_closeness(y_pred, y_true):\n",
    "    return K.mean(1 - K.abs(y_pred - y_true), axis=-1)\n",
    "keras.metrics.image_closeness = image_closeness\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load autoencoder and get decoder portion\n",
    "autoencoder = load_model(path_prefix + 'resources/autoencoder-v2-9k-epochs.h5') # transpose conv\n",
    "decoder = autoencoder.layers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze decoder weights\n",
    "# for layer in decoder.layers:\n",
    "#     layer.trainable = False\n",
    "# decoder.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bounding intermediate model outputs might yield better results\n",
    "def build_intermediate_model(input_shape, name='textencoder', encode_channels=[4, 8, 16, 32, 32]):\n",
    "    '''\n",
    "    Builds textual encoding model. This model's input is textual word2vec embedded vectors and outputs latent image size vectors.\n",
    "    '''\n",
    "    embedding_size = 64\n",
    "    gru_size = 1024\n",
    "    input_length = input_shape[0]\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    \n",
    "    embedded_seq, state_i = GRU(gru_size, return_state=True, name='gru')(input_layer)\n",
    "    state_i = Dense(1024)(state_i)\n",
    "    state_i = LeakyReLU()(state_i)\n",
    "\n",
    "    \n",
    "        \n",
    " \n",
    "    encoder_block = Reshape((32, 32, 1))(state_i)\n",
    "    \n",
    "    for index, channel in enumerate(encode_channels):\n",
    "        \n",
    "        shortcut = Conv2D(channel, 3, padding='same', trainable=False)(encoder_block)\n",
    "        encoder_block = BatchNormalization()(encoder_block)\n",
    "        encoder_block = LeakyReLU()(encoder_block)\n",
    "        encoder_block = Conv2D(channel, 3, padding='same')(encoder_block)    \n",
    "        \n",
    "        encoder_block = BatchNormalization()(encoder_block)\n",
    "        encoder_block = LeakyReLU()(encoder_block)\n",
    "        encoder_block = Conv2D(channel, 3, padding='same')(encoder_block)\n",
    "        \n",
    "        \n",
    "        encoder_block = Add()([encoder_block, shortcut])\n",
    "        \n",
    "        encoder_block = Conv2D(channel, 3, padding='same')(encoder_block)\n",
    "        encoder_block = LeakyReLU()(encoder_block)\n",
    "\n",
    "    output_layer = encoder_block\n",
    "    return Model(input_layer, output_layer, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_intermediate = build_intermediate_model((max_sequence_length, 100))\n",
    "model_intermediate.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = len(model_intermediate.layers)\n",
    "print(f'Model is {num_layers} layers deep.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image to latent model\n",
    "# input: (128,128,3)\n",
    "# output: (32,32,32)\n",
    "\n",
    "def build_image_to_latent_model(input_shape, name='image-to-latent', conv_filters=[8,16, 32, 64]):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    \n",
    "    for index, channel in enumerate(conv_filters):\n",
    "        \n",
    "        encoder_block = BatchNormalization()(input_layer if index == 0 else encoder_block)\n",
    "        encoder_block = LeakyReLU()(encoder_block)\n",
    "        encoder_block = Conv2D(channel, 3, padding='same')(encoder_block)    \n",
    "        \n",
    "        encoder_block = BatchNormalization()(encoder_block)\n",
    "        encoder_block = LeakyReLU()(encoder_block)\n",
    "        encoder_block = Conv2D(channel, 3, padding='same')(encoder_block)\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_block = Conv2D(channel, 3, padding='same', strides=2)(encoder_block)\n",
    "        encoder_block = LeakyReLU()(encoder_block)\n",
    "        \n",
    "    encoder_block = Conv2D(conv_filters[0], 3, padding='same', strides=1)(encoder_block)\n",
    "    encoder_block = LeakyReLU()(encoder_block)\n",
    "    output_layer = encoder_block\n",
    "    \n",
    "    \n",
    "    return Model(input_layer, output_layer, name=name)\n",
    "image_to_latent = build_image_to_latent_model((128,128,3), conv_filters=[16, 32])\n",
    "image_to_latent.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent to labels model\n",
    "# input: (32,32,32)\n",
    "# output: (12)\n",
    "\n",
    "def build_latent_to_labels_model(input_shape):\n",
    "    input_latent = Input(shape=input_shape)\n",
    "    x = Flatten()(input_latent)  \n",
    "\n",
    "    \n",
    "    x = Dense(32)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Dense(32)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Dense(16)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Dense(12, activation='softmax')(x)\n",
    "    \n",
    "    return Model(input_latent, x, name=\"latent-to-labels\")\n",
    "\n",
    "latent_to_labels = build_latent_to_labels_model((32,32,48))\n",
    "latent_to_labels.summary()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combined_model():\n",
    "    '''\n",
    "    Builds combined generator model. \n",
    "    '''\n",
    "    input_caption = Input(shape=(max_sequence_length, 100))\n",
    "    latent_representation = model_intermediate(input_caption)\n",
    "    \n",
    "    decoded_img = decoder(latent_representation)\n",
    "    \n",
    "    submodel_latent = image_to_latent(decoded_img)\n",
    "    submodel_latent = Concatenate()([submodel_latent, latent_representation])\n",
    "    labels_out = latent_to_labels(submodel_latent)\n",
    "    combined_model = Model(input_caption, [decoded_img, labels_out], name=\"text2img\")\n",
    "    return combined_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text2img = build_combined_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model_text2img.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "\n",
    "def caption_encoding(string):\n",
    "    '''\n",
    "    Returns encoded caption vector from string.\n",
    "    '''\n",
    "    caption_conv = []\n",
    "    for word in word_tokenize(string.lower()):\n",
    "        caption_conv.append(w2v_model[word])\n",
    "    caption_conv = np.array(caption_conv)\n",
    "    caption_conv.resize((max_sequence_length, 100))\n",
    "    return caption_conv\n",
    "\n",
    "def rgb_imshow(img, name):\n",
    "    '''\n",
    "    Shows bgr opencv image in rgb.\n",
    "    '''\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if name is not None:\n",
    "        plt.title('\\n'.join(textwrap.wrap(name, 20)), fontsize=8)\n",
    "    else:\n",
    "        plt.title(name)\n",
    "    plt.imshow(cv2.cvtColor((img).astype(np.float32), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "def show_converted(model, caption, noise_ratio=0.1, is_embedded=False):\n",
    "    '''\n",
    "    Sample from generative model using the noise ratio.\n",
    "    '''\n",
    "    figure(num=None, dpi=100, facecolor='w', edgecolor='k')\n",
    "    \n",
    "    caption_input = None\n",
    "    if not is_embedded:\n",
    "        caption_input = caption_encoding(caption) + noise_ratio * np.random.normal(dist_std, dist_std, (max_sequence_length, 100))\n",
    "    else:\n",
    "        caption_input = caption + noise_ratio * np.random.normal(dist_std, dist_std, (max_sequence_length, 100))\n",
    "\n",
    "    decoded_img  = model.predict(np.array([caption_input]), steps=None)[0]\n",
    "    print(np.shape(decoded_img))\n",
    "    rgb_imshow(np.reshape(decoded_img, (128,128,3)), caption)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    '''\n",
    "    Used to plot history after training.\n",
    "    '''\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(conv_channels=[8, 16, 32], dropout=False, residual=False, concat_captions=False):\n",
    "    if concat_captions:\n",
    "        text_input = Input(shape=(max_sequence_length, 100))\n",
    "    image_input = Input(shape=(128, 128, 3))\n",
    "\n",
    "    if concat_captions:\n",
    "        text_attn = Flatten()(text_input)\n",
    "\n",
    "    dropout_rate = 0.5\n",
    "\n",
    "    for index, channel in enumerate(conv_channels):\n",
    "        if residual:\n",
    "            shortcut = Conv2D(channel, 3, padding='same', trainable=False)(image_input if index == 0 else discriminator_block)\n",
    "        discriminator_block = BatchNormalization()(image_input if index == 0 else discriminator_block)\n",
    "        discriminator_block = LeakyReLU()(discriminator_block)\n",
    "        discriminator_block = Conv2D(channel, 3, padding='same')(discriminator_block)\n",
    "        if dropout:\n",
    "            discriminator_block = Dropout(dropout_rate)(discriminator_block)\n",
    "\n",
    "        discriminator_block = BatchNormalization()(discriminator_block)\n",
    "        discriminator_block = LeakyReLU()(discriminator_block)\n",
    "        discriminator_block = Conv2D(channel, 3, padding='same')(discriminator_block)\n",
    "        if dropout:\n",
    "            discriminator_block = Dropout(dropout_rate)(discriminator_block)\n",
    "        if residual:\n",
    "            discriminator_block = Add()([discriminator_block, shortcut])\n",
    "\n",
    "        discriminator_block = Conv2D(channel, 3, padding='same', strides=2)(discriminator_block)\n",
    "        discriminator_block = LeakyReLU()(discriminator_block)\n",
    "        if dropout:\n",
    "            discriminator_block = Dropout(dropout_rate)(discriminator_block)\n",
    "\n",
    "\n",
    "    discriminator_block = Flatten()(discriminator_block)\n",
    "    if concat_captions:\n",
    "        discriminator_block = Concatenate()([discriminator_block, text_attn])\n",
    "\n",
    "    dense = Dense(1, activation='sigmoid')(discriminator_block) #change to concatted if add text back\n",
    "\n",
    "    return Model(image_input, dense, name='Discriminator') #Model([text_input, image_input], dense, name='Discriminator')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan():\n",
    "    '''\n",
    "    Builds generative adversarial model for training.\n",
    "    '''\n",
    "    input_caption = Input(shape=(max_sequence_length, 100))\n",
    "    input_image = Input(shape=(128, 128, 3))\n",
    "    generated_image, generated_labels = model_text2img(input_caption)\n",
    "    discriminator_output = discriminator(generated_image) #discriminator([input_caption, generated_image])\n",
    "    \n",
    "\n",
    "    gan = Model(input_caption, [discriminator_output, generated_image, generated_labels])\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logic to train descriminator in adversarial model\n",
    "model_intermediate = build_intermediate_model((max_sequence_length, 100))\n",
    "model_text2img = build_combined_model()\n",
    "discriminator = build_discriminator(conv_channels=[4, 8, 16, 32, 64], dropout=True)\n",
    "d_adam = Adam(lr=0.00001, beta_1=0.0, beta_2=0.999)\n",
    "discriminator.compile(optimizer=d_adam, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "discriminator.trainable = False\n",
    "adversarial_net = build_gan()\n",
    "g_adam = Adam(lr=0.0001, beta_1=0.0, beta_2=0.999)\n",
    "adversarial_net.compile(optimizer=g_adam, loss=['binary_crossentropy', 'mse', 'binary_crossentropy'], loss_weights=[1, 10, 0.1], metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superclass_y = []\n",
    "for row in annot_list[:sample_count]:\n",
    "    superclass_y.append(ast.literal_eval(row[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superclass_y = np.array(superclass_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_loss_means = []\n",
    "d_acc_means = []\n",
    "a_loss_means = []\n",
    "t_loss_means = []\n",
    "classes_loss_means = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from keras.utils.generic_utils import Progbar\n",
    "from time import time\n",
    "\n",
    "\n",
    "\n",
    "# custom adversarial training logic\n",
    "BATCHSIZE = 32\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    num_data_pts = len(captions_X)\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(captions_X)\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(imgs_y)\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(caption_strings)\n",
    "\n",
    "    d_hist_loss = []\n",
    "    d_hist_acc = []\n",
    "    a_hist_loss = []\n",
    "    t_hist_loss = []\n",
    "    classes_hist_loss = []\n",
    "    print()\n",
    "    print(\"epoch {} of {}\".format(epoch+1, EPOCHS))\n",
    "    num_batches = int(num_data_pts // BATCHSIZE)\n",
    "    #print(\"number of batches: {}\".format(int(X.shape[0] // (BATCHSIZE))))\n",
    "#     progress_bar = Progbar(target=int(num_data_pts // (BATCHSIZE)))\n",
    "    minibatches_size = BATCHSIZE\n",
    "    start_time = time()\n",
    "    \n",
    "    for index in range(int(num_data_pts // (BATCHSIZE))):\n",
    "#         progress_bar.update(index)\n",
    "        \n",
    "        #print(f'{BATCHSIZE*index}:{(BATCHSIZE*(index+1))}')\n",
    "        images_real = imgs_y[BATCHSIZE*index:(BATCHSIZE*(index+1))]\n",
    "        captions_batch = captions_X[BATCHSIZE*index:(BATCHSIZE*(index+1))] + 0.1 * np.random.normal(dist_mean, dist_std, (BATCHSIZE, max_sequence_length, 100))\n",
    "        labels_fake = np.zeros([BATCHSIZE,1], dtype=np.float32)\n",
    "        labels_real = np.ones([BATCHSIZE,1], dtype=np.float32)        \n",
    "        superclass_real = superclass_y[BATCHSIZE*index:(BATCHSIZE*(index+1))]\n",
    "        \n",
    "        images_fake, classes_out = model_text2img.predict(captions_batch)\n",
    "        \n",
    "\n",
    "        \n",
    "        train_imgs = np.concatenate((images_real, images_fake))\n",
    "        train_captions = np.concatenate((captions_batch, captions_batch))\n",
    "        train_labels = np.concatenate((labels_real, labels_fake))\n",
    "        \n",
    "\n",
    " \n",
    "        np.random.seed(index)\n",
    "        np.random.shuffle(train_imgs)\n",
    "        np.random.seed(index)\n",
    "        np.random.shuffle(train_captions)\n",
    "        np.random.seed(index)\n",
    "        np.random.shuffle(train_labels)\n",
    "        \n",
    "        # train discrimiator every 20th epoch\n",
    "        if index % 20 == 0:\n",
    "            d_loss, d_acc = discriminator.train_on_batch(train_imgs, train_labels)\n",
    "            d_hist_loss.append(d_loss)\n",
    "            d_hist_acc.append(d_acc)\n",
    "\n",
    "            \n",
    "        a_loss_1, _, t_loss, t_loss_class, _, t_acc, classes_acc = adversarial_net.train_on_batch(captions_batch, [labels_real, images_real, superclass_real])\n",
    "        a_hist_loss.append(a_loss_1)\n",
    "        t_hist_loss.append(t_loss)\n",
    "        classes_hist_loss.append(t_loss_class)\n",
    "        #\n",
    "# ['loss',\n",
    "#  'Discriminator_loss',\n",
    "#  'text2img_loss',\n",
    "#  'text2img_loss',\n",
    "#  'Discriminator_binary_accuracy',\n",
    "#  'text2img_binary_accuracy',\n",
    "#  'text2img_binary_accuracy_1']\n",
    "        #\n",
    "        #\n",
    "\n",
    "    classes_loss_means.append(np.mean(t_loss_class))\n",
    "    t_loss_means.append(np.mean(t_hist_loss))\n",
    "    d_loss_means.append(np.mean(d_hist_loss))\n",
    "    a_loss_means.append(np.mean(a_hist_loss))\n",
    "    d_acc_means.append(np.mean(d_hist_acc))\n",
    "    print(' ' + str(np.mean(d_hist_acc)))\n",
    "\n",
    "\n",
    "\n",
    "# adversarial_net.save_weights('.\\\\gan-v1\\\\gan-2k-epochs-weights.h5')\n",
    "# model_text2img.save_weights('.\\\\gan-v1\\\\text2img-2k-epochs-weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For use on Nautilus only\n",
    "import boto3\n",
    "import io\n",
    "def put_object(obj_file_path, obj_key):\n",
    "    if path_prefix != '/coco2017/':\n",
    "        return\n",
    "    try:\n",
    "        with open('/asak/aws-secret-access-key', 'r') as asak, open('/aaki/aws-access-key-id', 'r') as aaki:\n",
    "            with open (obj_file_path) as put_file:\n",
    "                session = boto3.Session(aws_secret_access_key=asak.read().strip(), aws_access_key_id = aaki.read().strip())\n",
    "                s3 = session.resource('s3')\n",
    "                client = session.client('s3', endpoint_url='https://s3.nautilus.optiputer.net')\n",
    "\n",
    "                val = client.put_object(Body=obj_file, Bucket='kevin-bucket', Key=obj_key)\n",
    "    except Exception as e:\n",
    "        print ('Error putting:')\n",
    "        print (e)\n",
    "        print ('Response:')\n",
    "        print (val)\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text2img.save_weights(path_prefix + 'results/text2img-prp-100epochs.h5')\n",
    "\n",
    "put_object(path_prefix + 'results/text2img-prp-100epochs.h5', 'gan-weights-100epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.title('Discriminator Loss')\n",
    "plt.plot(d_loss_means)\n",
    "plt.savefig(path_prefix + 'results/discriminator_loss-prp-100epochs.png')\n",
    "plt.show()\n",
    "put_object(path_prefix + 'results/discriminator_loss-prp-100epochs.png', 'd_loss-100epochs')\n",
    "\n",
    "\n",
    "plt.title('Discriminator Accuracy')\n",
    "plt.plot(d_acc_means)\n",
    "plt.savefig(path_prefix + 'results/discriminator_acc-prp-100epochs.png')\n",
    "plt.show()\n",
    "put_object(path_prefix + 'results/discriminator_acc-prp-100epochs.png', 'd_acc-100epochs')\n",
    "\n",
    "plt.title('Adversarial Loss')\n",
    "plt.plot(a_loss_means)\n",
    "plt.savefig(path_prefix + 'results/adversarial_loss-prp-100epochs.png')\n",
    "plt.show()\n",
    "put_object(path_prefix + 'results/adversarial_loss-prp-100epochs.png', 'adv_loss-100epochs')\n",
    "\n",
    "plt.title('Text To Image Loss')\n",
    "plt.plot(t_loss_means)\n",
    "plt.savefig(path_prefix + 'results/text2img_loss-prp-100epochs.png')\n",
    "plt.show()\n",
    "put_object(path_prefix + 'results/text2img_loss-prp-100epochs.png', 't2i_loss-100epochs')\n",
    "\n",
    "plt.title('Classes Loss')\n",
    "plt.plot(classes_loss_means)\n",
    "plt.savefig(path_prefix + 'results/classes_loss-prp-100epochs.png')\n",
    "plt.show()\n",
    "put_object(path_prefix + 'results/classes_loss-prp-100epochs.png', 'classes_loss-100epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
