{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, merge, Concatenate, Dense, Dropout, Conv2D, Add, Dot, Lambda, Conv2DTranspose, Dot, Activation, Reshape, BatchNormalization, UpSampling2D, AveragePooling2D, GlobalAveragePooling2D, Multiply, Softmax, LeakyReLU, Flatten, MaxPool2D, MaxPool3D, Embedding, GRU\n",
    "from keras.layers.convolutional import Convolution2D, UpSampling2D, SeparableConv2D, UpSampling3D\n",
    "from keras.layers import PReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "import keras.backend as K\n",
    "from keras import layers\n",
    "from keras import activations\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all captions\n",
    "captions = pd.read_csv('./coco-captions-2017-clean.csv')\n",
    "annot_list = captions.values[:1600]\n",
    "data = [] \n",
    "corpus_dict = {}\n",
    "# iterate through each caption \n",
    "max_sequence_length = float(\"-inf\")\n",
    "for row in annot_list: \n",
    "    temp = [] \n",
    "      \n",
    "    #tokenize the captions into words \n",
    "    for j in word_tokenize(row[2]): \n",
    "        temp.append(j.lower()) \n",
    "        corpus_dict[j.lower()] = 0\n",
    "    if len(temp) > max_sequence_length:\n",
    "        max_sequence_length = len(temp)\n",
    "    data.append(temp) \n",
    "\n",
    "for index, word in enumerate(corpus_dict.keys()):\n",
    "    corpus_dict[word] = index + 1\n",
    "    \n",
    "corpus_size = len(corpus_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load external text embedding model (word2vec)\n",
    "w2v_model = Word2Vec.load('./text_encoding.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# encode all our captions\n",
    "captions_X = []\n",
    "caption_strings = []\n",
    "\n",
    "for row in annot_list:\n",
    "    caption_conv = []\n",
    "    for word in word_tokenize(row[2].lower()):\n",
    "        caption_conv.append(w2v_model[word])\n",
    "    captions_X.append(np.array(caption_conv))\n",
    "    caption_strings.append(row[2].lower())\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad with 0-vectors to max_sequence_length\n",
    "for i in range(len(captions_X)):\n",
    "    captions_X[i].resize((max_sequence_length, 100))\n",
    "captions_X = np.array(captions_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(file_path):\n",
    "    img_paths = glob.glob(file_path + '/*')\n",
    "    data = []\n",
    "    for img_path in img_paths:\n",
    "        \n",
    "        data.append(cv2.imread(img_path) / 255.0)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_all = data_loader('./cleaned-data/')\n",
    "imgs_y = imgs_all[:1600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom metric to avoid binary accuracy rounding\n",
    "import keras.metrics\n",
    "def image_closeness(y_pred, y_true):\n",
    "    return K.mean(1 - K.abs(y_pred - y_true), axis=-1)\n",
    "keras.metrics.image_closeness = image_closeness\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load autoencoder and get decoder portion\n",
    "autoencoder = load_model('./autoencoder-v2-models/autoencoder-v2-9k-epochs.h5') # transpose conv\n",
    "decoder = autoencoder.layers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze decoder weights\n",
    "for layer in decoder.layers:\n",
    "    layer.trainable = False\n",
    "decoder.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_intermediate_model(input_shape, name='textencoder', encode_channels=[4, 8, 16, 32, 32]):\n",
    "    embedding_size = 64\n",
    "    gru_size = 1024\n",
    "    input_length = max_sequence_length\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    \n",
    "    embedded_seq, state_i = GRU(gru_size, return_state=True, name='gru')(input_layer)\n",
    "    state_i = Dense(1024)(state_i)\n",
    "    state_i = LeakyReLU()(state_i)\n",
    "\n",
    "    \n",
    "        \n",
    " \n",
    "    encoder_block = Reshape((32, 32, 1))(state_i)\n",
    "    \n",
    "    for index, channel in enumerate(encode_channels):\n",
    "        \n",
    "        shortcut = Conv2D(channel, 3, padding='same', trainable=False)(encoder_block)\n",
    "        encoder_block = BatchNormalization()(encoder_block)\n",
    "        encoder_block = LeakyReLU()(encoder_block)\n",
    "        encoder_block = Conv2D(channel, 3, padding='same')(encoder_block)    \n",
    "        \n",
    "        encoder_block = BatchNormalization()(encoder_block)\n",
    "        encoder_block = LeakyReLU()(encoder_block)\n",
    "        encoder_block = Conv2D(channel, 3, padding='same')(encoder_block)\n",
    "        \n",
    "        \n",
    "        encoder_block = Add()([encoder_block, shortcut])\n",
    "        \n",
    "        encoder_block = Conv2D(channel, 3, padding='same')(encoder_block)\n",
    "        encoder_block = LeakyReLU()(encoder_block)\n",
    "\n",
    "    output_layer = encoder_block\n",
    "    return Model(input_layer, output_layer, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_intermediate = build_intermediate_model((max_sequence_length, 100))\n",
    "model_intermediate.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = len(model_intermediate.layers)\n",
    "print(f'Model is {num_layers} layers deep.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combined_model():\n",
    "    input_caption = Input(shape=(27, 100))\n",
    "    latent_representation = model_intermediate(input_caption)\n",
    "    decoded_img = decoder(latent_representation)\n",
    "    autoencoder = Model(input_caption, decoded_img, name=\"text2img\")\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text2img = build_combined_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_text2img.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text2img.compile(optimizer='adam', loss='mse', metrics=[image_closeness, 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = load_model('./text2img-models-v3/text2img-3k-epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = m.fit(captions_X, imgs_y, epochs=1, batch_size=32, shuffle=True)\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range (1, 3):\n",
    "    history = model_text2img.fit(captions_X, imgs_y, epochs=1000, batch_size=32, shuffle=True)\n",
    "    print(str(i) + 'k epochs~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    model_text2img.save('.\\\\text2img-models-v5\\\\text2img-' + str(i) + 'k-epochs.h5')\n",
    "    plot_history(history)\n",
    "#     show_converted(4)\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_text2img = load_model('./text2img-models-v2/text2img-1k-epochs.h5')\n",
    "model_text2img.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for generating figures\n",
    "model_text2img_t = load_model('./text2img-models-v4/text2img-2k-epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for caption in caption_strings:\n",
    "    if 'woman on a surfboard' in caption:\n",
    "        print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "# figure(num=None, figsize=(10, 10), dpi=200, facecolor='w', edgecolor='k')\n",
    "def rgb_imshow(img, name):\n",
    "#     figure(num=None, figsize=(10, 10), dpi=200, facecolor='w', edgecolor='k')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if name is not None:\n",
    "        plt.title('\\n'.join(textwrap.wrap(name, 20)), fontsize=8)\n",
    "    else:\n",
    "        plt.title(name)\n",
    "    plt.imshow(cv2.cvtColor((img).astype(np.float32), cv2.COLOR_BGR2RGB))\n",
    "#     plt.show()\n",
    "\n",
    "def show_converted(model, caption):\n",
    "    \n",
    "    \n",
    "\n",
    "#     caption_input = testcaptions[0] \n",
    "# THE VAULT\n",
    "# 1604, 1610, 1628, 1642, 1748, 1771, 1772, 1860, 1926, 13018, 13145, 13189, 13435, 13517\n",
    "#     vault = [0, 4, 5, 69]\n",
    "    figure(num=None, dpi=300, facecolor='w', edgecolor='k')\n",
    "\n",
    "    caption_input = caption_encoding(caption)\n",
    "    target_image = testimgs[0]\n",
    "    decoded_img = model.predict(np.array([caption_input]), steps=None)[0]\n",
    "    rgb_imshow(decoded_img, caption)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "#     plt.savefig(f'./testing-final-images/{caption}.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# show_converted(2)\n",
    "# show_converted(4)\n",
    "# show_converted(18)\n",
    "# show_converted(65)\n",
    "# for i in range(65, 70):\n",
    "\n",
    "# [68, 124, 47, 38]\n",
    "testimgs = [imgs_y[68], imgs_y[124], imgs_y[47], imgs_y[38]]\n",
    "testcaptions = [captions_X[68], captions_X[124], captions_X[47], captions_X[38]]\n",
    "teststrings = [caption_strings[68], caption_strings[124], caption_strings[47], caption_strings[38]]\n",
    "\n",
    "caps = \\\n",
    "[\n",
    "    'A herd of animals walking across a dry grass field.',\\\n",
    "    'A man riding skis on top of a snow covered slope.',\\\n",
    "    'A man riding a wave on top of a surfboard',\\\n",
    "    'A little kid playing second base at a baseball game.',\\\n",
    "    'A young woman on a surfboard surfing on a wave',\\\n",
    "    'A cat is sitting inside of a suitcase.',\\\n",
    "    'A large propeller airplane flying through a blue sky.',\\\n",
    "    'A man flying a kite above in a blue sky.',\\\n",
    "    'A herd of animals walking across a dry grass field.',\\\n",
    "]\n",
    "\n",
    "for cap in caps:\n",
    "    show_converted(model_text2img_t, cap)\n",
    "# show_converted(3, model_text2img_u, caption=caption_encoding(caption_strings[img_num]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# decoded_img1 = model_text2img.predict(np.array([captions_X[4]]))[0]\n",
    "# decoded_img2 = model_text2img.predict(np.array([captions_X[5]]))[0]\n",
    "\n",
    "# flat_img = np.array(decoded_img2 - decoded_img1)\n",
    "# flat_img = flat_img.flatten()\n",
    "# nonzero_elems = [x for x in flat_img if x != 0]\n",
    "# print (len(decoded_img1.flatten()))\n",
    "# print (len(nonzero_elems))\n",
    "# print (np.max(flat_img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_encoding(string):\n",
    "    caption_conv = []\n",
    "    for word in word_tokenize(string.lower()):\n",
    "        caption_conv.append(w2v_model[word])\n",
    "    caption_conv = np.array(caption_conv)\n",
    "    caption_conv.resize((27, 100))\n",
    "    return caption_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(model_text2img.predict(np.array([captions_X[0]]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pad_text(text):\n",
    "    paddings = tf.constant([[0, 0], [0, 52]])\n",
    "    text = tf.pad(text, paddings, 'CONSTANT')\n",
    "    return text\n",
    "\n",
    "def build_discriminator(encode_channels=[8, 16, 32], dropout=False, residual=False, concat_captions=False):\n",
    "    if concat_captions:\n",
    "        text_input = Input(shape=(27, 100))\n",
    "    image_input = Input(shape=(128, 128, 3))\n",
    "    \n",
    "    if concat_captions:\n",
    "        text_attn = Flatten()(text_input)\n",
    "    \n",
    "    \n",
    "    for index, channel in enumerate(encode_channels):\n",
    "        if residual:\n",
    "            shortcut = Conv2D(channel, 3, padding='same', trainable=False)(image_input if index == 0 else encoder_block)\n",
    "        encoder_block = BatchNormalization()(image_input if index == 0 else encoder_block)\n",
    "        encoder_block = LeakyReLU()(encoder_block)\n",
    "        encoder_block = Conv2D(channel, 3, padding='same')(encoder_block)\n",
    "        if dropout:\n",
    "            encoder_block = Dropout(0.2)(encoder_block)\n",
    "        \n",
    "        encoder_block = BatchNormalization()(encoder_block)\n",
    "        encoder_block = LeakyReLU()(encoder_block)\n",
    "        encoder_block = Conv2D(channel, 3, padding='same')(encoder_block)\n",
    "        if dropout:\n",
    "            encoder_block = Dropout(0.2)(encoder_block)\n",
    "        if residual:\n",
    "            encoder_block = Add()([encoder_block, shortcut])\n",
    "        \n",
    "        encoder_block = Conv2D(channel, 3, padding='same', strides=2)(encoder_block)\n",
    "        encoder_block = LeakyReLU()(encoder_block)\n",
    "        if dropout:\n",
    "            encoder_block = Dropout(0.2)(encoder_block)\n",
    "        \n",
    "    \n",
    "    encoder_block = Flatten()(encoder_block)\n",
    "    if concat_captions:\n",
    "        encoder_block = Concatenate()([encoder_block, text_attn])\n",
    "    \n",
    "    dense = Dense(1, activation='sigmoid')(encoder_block) #change to concatted if add text back\n",
    "    \n",
    "    return Model(image_input, dense, name='Discriminator') #Model([text_input, image_input], dense, name='Discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan():\n",
    "    input_caption = Input(shape=(27, 100))\n",
    "    input_image = Input(shape=(128, 128, 3))\n",
    "    generated_image = model_text2img(input_caption)\n",
    "    discriminator_output = discriminator(generated_image) #discriminator([input_caption, generated_image])\n",
    "    \n",
    "\n",
    "    gan = Model(input_caption, [discriminator_output, generated_image])\n",
    "    return gan\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logic to train descriminator in adversarial model\n",
    "model_intermediate = build_intermediate_model((27, 100))\n",
    "model_text2img = build_combined_model()\n",
    "discriminator = build_discriminator()\n",
    "d_adam = keras.optimizers.adam(lr=0.00001, beta_1=0.0, beta_2=0.999)\n",
    "discriminator.compile(optimizer=d_adam, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "discriminator.trainable = False\n",
    "adversarial_net = build_gan()\n",
    "g_adam = keras.optimizers.adam(lr=0.0001, beta_1=0.0, beta_2=0.999)\n",
    "adversarial_net.compile(optimizer=g_adam, loss=['binary_crossentropy', 'mse'], loss_weights=[1, 10], metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_X = captions_X[:128]\n",
    "imgs_y = imgs_y[:128]\n",
    "caption_strings = caption_strings[:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.generic_utils import Progbar\n",
    "from time import time\n",
    "\n",
    "d_loss_means = []\n",
    "d_acc_means = []\n",
    "a_loss_means = []\n",
    "t_loss_means = []\n",
    "\n",
    "# custom adversarial training logic\n",
    "BATCHSIZE = 32\n",
    "EPOCHS = 1000\n",
    "for epoch in range(EPOCHS):\n",
    "    num_data_pts = len(captions_X)\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(captions_X)\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(imgs_y)\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(caption_strings)\n",
    "\n",
    "    d_hist_loss = []\n",
    "    d_hist_acc = []\n",
    "    a_hist_loss = []\n",
    "    t_hist_loss = []\n",
    "    print()\n",
    "    print(\"epoch {} of {}\".format(epoch+1, EPOCHS))\n",
    "    num_batches = int(num_data_pts // BATCHSIZE)\n",
    "    #print(\"number of batches: {}\".format(int(X.shape[0] // (BATCHSIZE))))\n",
    "    progress_bar = Progbar(target=int(num_data_pts // (BATCHSIZE)))\n",
    "    minibatches_size = BATCHSIZE\n",
    "    start_time = time()\n",
    "    \n",
    "    for index in range(int(num_data_pts // (BATCHSIZE))):\n",
    "        progress_bar.update(index)\n",
    "        \n",
    "        #print(f'{BATCHSIZE*index}:{(BATCHSIZE*(index+1))}')\n",
    "        images_real = imgs_y[BATCHSIZE*index:(BATCHSIZE*(index+1))]\n",
    "        captions_batch = captions_X[BATCHSIZE*index:(BATCHSIZE*(index+1))]\n",
    "        #print(edges_batch.shape)\n",
    "        labels_fake = np.zeros([BATCHSIZE,1], dtype=np.float32)\n",
    "        labels_real = np.ones([BATCHSIZE,1], dtype=np.float32)\n",
    "        \n",
    "        images_fake = model_text2img.predict(captions_batch)\n",
    "        train_imgs = np.concatenate((images_real, images_fake))\n",
    "        train_captions = np.concatenate((captions_batch, captions_batch))\n",
    "        train_labels = np.concatenate((labels_real, labels_fake))\n",
    "        \n",
    "\n",
    " \n",
    "        np.random.seed(index)\n",
    "        np.random.shuffle(train_imgs)\n",
    "        np.random.seed(index)\n",
    "        np.random.shuffle(train_captions)\n",
    "        np.random.seed(index)\n",
    "        np.random.shuffle(train_labels)\n",
    "        \n",
    "        # train discrimiator every 4th epoch\n",
    "        if index % 4 == 0:\n",
    "            d_loss, d_acc = discriminator.train_on_batch(train_imgs, train_labels)\n",
    "            d_hist_loss.append(d_loss)\n",
    "            d_hist_acc.append(d_acc)\n",
    "\n",
    "        \n",
    "        \n",
    "        a_loss_1, _, t_loss, _, t_acc = adversarial_net.train_on_batch(captions_batch, [labels_real, images_real])\n",
    "        a_hist_loss.append(a_loss_1)\n",
    "        t_hist_loss.append(t_loss)\n",
    "        \n",
    "\n",
    "    \n",
    "    t_loss_means.append(np.mean(t_hist_loss))\n",
    "    d_loss_means.append(np.mean(d_hist_loss))\n",
    "    a_loss_means.append(np.mean(a_hist_loss))\n",
    "    d_acc_means.append(np.mean(d_hist_acc))\n",
    "    print(np.mean(d_hist_acc))\n",
    "\n",
    "\n",
    "\n",
    "# adversarial_net.save_weights('.\\\\gan-v1\\\\gan-2k-epochs-weights.h5')\n",
    "# model_text2img.save_weights('.\\\\gan-v1\\\\text2img-2k-epochs-weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title('Discriminator Loss')\n",
    "plt.plot(d_loss_means)\n",
    "# plt.savefig('./gan-v1/discriminator_loss-2k')\n",
    "plt.show()\n",
    "plt.title('Discriminator Accuracy')\n",
    "plt.plot(d_acc_means)\n",
    "# plt.savefig('./gan-v1/discriminator_acc-2k')\n",
    "plt.show()\n",
    "plt.title('Adversarial Loss')\n",
    "plt.plot(a_loss_means)\n",
    "# plt.savefig('./gan-v1/adversarial_loss-2k')\n",
    "plt.show()\n",
    "plt.title('Text To Image Loss')\n",
    "plt.plot(t_loss_means)\n",
    "# plt.savefig('./gan-v1/text2img_loss-2k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vault = [68, 124, 47, 38]\n",
    "for img_num in vault:\n",
    "    print(img_num)\n",
    "    rgb_imshow(imgs_y[img_num], caption_strings[img_num]) \n",
    "    plt.show()\n",
    "    show_converted(img_num, model_text2img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_intermediate = build_intermediate_model((27, 100))\n",
    "model_text2img = build_combined_model()\n",
    "#model_text2img.compile(optimizer='adam', loss='mse', metrics=[image_closeness, 'accuracy'])\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "discriminator.compile(optimizer=d_adam, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "discriminator.trainable = False\n",
    "adversarial_net = build_gan()\n",
    "adversarial_net.compile(optimizer=g_adam, loss=['binary_crossentropy', 'mse'], loss_weights=[1, 10], metrics=['binary_accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
