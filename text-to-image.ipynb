{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import ast\n",
    "import glob\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Concatenate, Dense, Dropout, Conv2D, Add, Dot, Lambda, Conv2DTranspose, Dot, Activation, Reshape, BatchNormalization, UpSampling2D, AveragePooling2D, GlobalAveragePooling2D, Multiply, Softmax, LeakyReLU, Flatten, MaxPool2D, MaxPool3D, Embedding, GRU\n",
    "from tensorflow.keras.layers import Convolution2D, UpSampling2D, SeparableConv2D, UpSampling3D\n",
    "from tensorflow.keras.layers import PReLU, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print('GPU is available: ', tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] data_percentage\n",
      "ipykernel_launcher.py: error: argument data_percentage: invalid float value: '/Users/kevin/Library/Jupyter/runtime/kernel-5d20717d-9c87-41ad-aad6-5041a6564c7b.json'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('data_percentage', type=float)\n",
    "args = parser.parse_args()\n",
    "data_percentage = args.data_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/coco2017/results/coco-captions-with-categories.csv' does not exist: b'/coco2017/results/coco-captions-with-categories.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-9306a6ef5d89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load all captions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/coco2017/results/coco-captions-with-categories.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msample_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_percentage\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mannot_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msample_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/coco2017/results/coco-captions-with-categories.csv' does not exist: b'/coco2017/results/coco-captions-with-categories.csv'"
     ]
    }
   ],
   "source": [
    "# load all captions\n",
    "captions = pd.read_csv('/coco2017/results/coco-captions-with-categories.csv')\n",
    "sample_count = round(data_percentage * len(captions.values))\n",
    "annot_list = captions.values[:sample_count]\n",
    "data = [] \n",
    "corpus_dict = {}\n",
    "# iterate through each caption \n",
    "max_sequence_length = float(\"-inf\")\n",
    "for row in annot_list: \n",
    "      \n",
    "    captions = row[1].split('|')[:5]\n",
    "    \n",
    "    for caption in captions:\n",
    "        temp = []\n",
    "        #tokenize the captions into words \n",
    "        for j in word_tokenize(caption): \n",
    "            temp.append(j.lower()) \n",
    "            corpus_dict[j.lower()] = 0\n",
    "        if len(temp) > max_sequence_length:\n",
    "            max_sequence_length = len(temp)\n",
    "        data.append(temp) \n",
    "\n",
    "for index, word in enumerate(corpus_dict.keys()):\n",
    "    corpus_dict[word] = index + 1\n",
    "    \n",
    "corpus_size = len(corpus_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load external text embedding model (word2vec)\n",
    "w2v_model = Word2Vec.load('/coco2017/resources/text_encoding_ac.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# encode all our captions\n",
    "captions_X = []\n",
    "caption_strings = []\n",
    "\n",
    "for row in annot_list[:sample_count]:\n",
    "    captions = row[1].split('|')[:1]\n",
    "    captions_list = []\n",
    "    captions_X_list = []\n",
    "    for caption in captions:\n",
    "        caption_conv = []\n",
    "#         print(caption)\n",
    "        for word in word_tokenize(caption.lower()):\n",
    "            caption_conv.append(w2v_model[word])\n",
    "        captions_X_list.append(np.array(caption_conv))\n",
    "        captions_list.append(caption.lower())\n",
    "    captions_X.append(captions_X_list)\n",
    "    caption_strings.append(captions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad with 0-vectors to max_sequence_length\n",
    "for i in range(len(captions_X)):\n",
    "    for j in range(len(captions_X[i])):\n",
    "        captions_X[i][j].resize((max_sequence_length, 100), refcheck=False)\n",
    "captions_X = np.array(captions_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to create the random distribution at training time\n",
    "dist_mean = np.mean(captions_X)\n",
    "dist_std = np.std(captions_X)\n",
    "print(dist_mean, dist_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_X = np.reshape(captions_X, (sample_count, max_sequence_length, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_loader(file_path):\n",
    "    '''\n",
    "    Loads and normalizes all image files in the directory file_path.\n",
    "    Returns np.array of normalized image matrices.\n",
    "    '''\n",
    "    img_paths = glob.glob(file_path + '/*')\n",
    "    data = []\n",
    "    for img_path in img_paths:\n",
    "        data.append(cv2.imread(img_path) / 255.0)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads in data and subsets it to the first 1600 samples\n",
    "# img_paths = glob.glob('./cleaned-data/00000000[0-9]*.jpg')\n",
    "# training_imgs_num = len(img_paths)\n",
    "# print(f'loaded a total of {training_imgs_num} imgs')\n",
    "imgs_all = data_loader('/coco2017/cleaned-data/')\n",
    "imgs_y = imgs_all[:sample_count]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(caption_strings[0])\n",
    "# plt.imshow(imgs_y[0])\n",
    "print(len(caption_strings))\n",
    "print(len(captions_X))\n",
    "print(len(imgs_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom metric to avoid binary accuracy rounding\n",
    "import keras.metrics\n",
    "def image_closeness(y_pred, y_true):\n",
    "    return K.mean(1 - K.abs(y_pred - y_true), axis=-1)\n",
    "keras.metrics.image_closeness = image_closeness\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load autoencoder and get decoder portion\n",
    "autoencoder = load_model('/coco2017/resources/autoencoder-v2-9k-epochs.h5') # transpose conv\n",
    "decoder = autoencoder.layers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze decoder weights\n",
    "# for layer in decoder.layers:\n",
    "#     layer.trainable = False\n",
    "# decoder.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bounding intermediate model outputs might yield better results\n",
    "def build_intermediate_model(input_shape, name='textencoder', encode_channels=[4, 8, 16, 32, 32]):\n",
    "    '''\n",
    "    Builds textual encoding model. This model's input is textual word2vec embedded vectors and outputs latent image size vectors.\n",
    "    '''\n",
    "    embedding_size = 64\n",
    "    gru_size = 1024\n",
    "    input_length = input_shape[0]\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    \n",
    "    embedded_seq, state_i = GRU(gru_size, return_state=True, name='gru')(input_layer)\n",
    "    state_i = Dense(1024)(state_i)\n",
    "    state_i = LeakyReLU()(state_i)\n",
    "\n",
    "    \n",
    "        \n",
    " \n",
    "    encoder_block = Reshape((32, 32, 1))(state_i)\n",
    "    \n",
    "    for index, channel in enumerate(encode_channels):\n",
    "        \n",
    "        shortcut = Conv2D(channel, 3, padding='same', trainable=False)(encoder_block)\n",
    "        encoder_block = BatchNormalization()(encoder_block)\n",
    "        encoder_block = LeakyReLU()(encoder_block)\n",
    "        encoder_block = Conv2D(channel, 3, padding='same')(encoder_block)    \n",
    "        \n",
    "        encoder_block = BatchNormalization()(encoder_block)\n",
    "        encoder_block = LeakyReLU()(encoder_block)\n",
    "        encoder_block = Conv2D(channel, 3, padding='same')(encoder_block)\n",
    "        \n",
    "        \n",
    "        encoder_block = Add()([encoder_block, shortcut])\n",
    "        \n",
    "        encoder_block = Conv2D(channel, 3, padding='same')(encoder_block)\n",
    "        encoder_block = LeakyReLU()(encoder_block)\n",
    "\n",
    "    output_layer = encoder_block\n",
    "    return Model(input_layer, output_layer, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_intermediate = build_intermediate_model((max_sequence_length, 100))\n",
    "model_intermediate.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = len(model_intermediate.layers)\n",
    "print(f'Model is {num_layers} layers deep.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image to latent model\n",
    "# input: (128,128,3)\n",
    "# output: (32,32,32)\n",
    "\n",
    "def build_image_to_latent_model(input_shape, name='image-to-latent', conv_filters=[8,16, 32, 64]):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    \n",
    "    for index, channel in enumerate(conv_filters):\n",
    "        \n",
    "        encoder_block = BatchNormalization()(input_layer if index == 0 else encoder_block)\n",
    "        encoder_block = LeakyReLU()(encoder_block)\n",
    "        encoder_block = Conv2D(channel, 3, padding='same')(encoder_block)    \n",
    "        \n",
    "        encoder_block = BatchNormalization()(encoder_block)\n",
    "        encoder_block = LeakyReLU()(encoder_block)\n",
    "        encoder_block = Conv2D(channel, 3, padding='same')(encoder_block)\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_block = Conv2D(channel, 3, padding='same', strides=2)(encoder_block)\n",
    "        encoder_block = LeakyReLU()(encoder_block)\n",
    "        \n",
    "    encoder_block = Conv2D(conv_filters[0], 3, padding='same', strides=1)(encoder_block)\n",
    "    encoder_block = LeakyReLU()(encoder_block)\n",
    "    output_layer = encoder_block\n",
    "    \n",
    "    \n",
    "    return Model(input_layer, output_layer, name=name)\n",
    "image_to_latent = build_image_to_latent_model((128,128,3), conv_filters=[16, 32])\n",
    "image_to_latent.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent to labels model\n",
    "# input: (32,32,32)\n",
    "# output: (12)\n",
    "\n",
    "def build_latent_to_labels_model(input_shape):\n",
    "    input_latent = Input(shape=input_shape)\n",
    "    x = Flatten()(input_latent)  \n",
    "\n",
    "    \n",
    "    x = Dense(32)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Dense(32)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Dense(16)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Dense(12, activation='softmax')(x)\n",
    "    \n",
    "    return Model(input_latent, x, name=\"latent-to-labels\")\n",
    "\n",
    "latent_to_labels = build_latent_to_labels_model((32,32,48))\n",
    "latent_to_labels.summary()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combined_model():\n",
    "    '''\n",
    "    Builds combined generator model. \n",
    "    '''\n",
    "    input_caption = Input(shape=(max_sequence_length, 100))\n",
    "    latent_representation = model_intermediate(input_caption)\n",
    "    \n",
    "    decoded_img = decoder(latent_representation)\n",
    "    \n",
    "    submodel_latent = image_to_latent(decoded_img)\n",
    "    submodel_latent = Concatenate()([submodel_latent, latent_representation])\n",
    "    labels_out = latent_to_labels(submodel_latent)\n",
    "    combined_model = Model(input_caption, [decoded_img, labels_out], name=\"text2img\")\n",
    "    return combined_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text2img = build_combined_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model_text2img.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "\n",
    "def caption_encoding(string):\n",
    "    '''\n",
    "    Returns encoded caption vector from string.\n",
    "    '''\n",
    "    caption_conv = []\n",
    "    for word in word_tokenize(string.lower()):\n",
    "        caption_conv.append(w2v_model[word])\n",
    "    caption_conv = np.array(caption_conv)\n",
    "    caption_conv.resize((max_sequence_length, 100))\n",
    "    return caption_conv\n",
    "\n",
    "def rgb_imshow(img, name):\n",
    "    '''\n",
    "    Shows bgr opencv image in rgb.\n",
    "    '''\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if name is not None:\n",
    "        plt.title('\\n'.join(textwrap.wrap(name, 20)), fontsize=8)\n",
    "    else:\n",
    "        plt.title(name)\n",
    "    plt.imshow(cv2.cvtColor((img).astype(np.float32), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "def show_converted(model, caption, noise_ratio=0.1, is_embedded=False):\n",
    "    '''\n",
    "    Sample from generative model using the noise ratio.\n",
    "    '''\n",
    "    figure(num=None, dpi=100, facecolor='w', edgecolor='k')\n",
    "    \n",
    "    caption_input = None\n",
    "    if not is_embedded:\n",
    "        caption_input = caption_encoding(caption) + noise_ratio * np.random.normal(dist_std, dist_std, (max_sequence_length, 100))\n",
    "    else:\n",
    "        caption_input = caption + noise_ratio * np.random.normal(dist_std, dist_std, (max_sequence_length, 100))\n",
    "\n",
    "    decoded_img  = model.predict(np.array([caption_input]), steps=None)[0]\n",
    "    print(np.shape(decoded_img))\n",
    "    rgb_imshow(np.reshape(decoded_img, (128,128,3)), caption)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    '''\n",
    "    Used to plot history after training.\n",
    "    '''\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(conv_channels=[8, 16, 32], dropout=False, residual=False, concat_captions=False):\n",
    "    if concat_captions:\n",
    "        text_input = Input(shape=(max_sequence_length, 100))\n",
    "    image_input = Input(shape=(128, 128, 3))\n",
    "\n",
    "    if concat_captions:\n",
    "        text_attn = Flatten()(text_input)\n",
    "\n",
    "    dropout_rate = 0.5\n",
    "\n",
    "    for index, channel in enumerate(conv_channels):\n",
    "        if residual:\n",
    "            shortcut = Conv2D(channel, 3, padding='same', trainable=False)(image_input if index == 0 else discriminator_block)\n",
    "        discriminator_block = BatchNormalization()(image_input if index == 0 else discriminator_block)\n",
    "        discriminator_block = LeakyReLU()(discriminator_block)\n",
    "        discriminator_block = Conv2D(channel, 3, padding='same')(discriminator_block)\n",
    "        if dropout:\n",
    "            discriminator_block = Dropout(dropout_rate)(discriminator_block)\n",
    "\n",
    "        discriminator_block = BatchNormalization()(discriminator_block)\n",
    "        discriminator_block = LeakyReLU()(discriminator_block)\n",
    "        discriminator_block = Conv2D(channel, 3, padding='same')(discriminator_block)\n",
    "        if dropout:\n",
    "            discriminator_block = Dropout(dropout_rate)(discriminator_block)\n",
    "        if residual:\n",
    "            discriminator_block = Add()([discriminator_block, shortcut])\n",
    "\n",
    "        discriminator_block = Conv2D(channel, 3, padding='same', strides=2)(discriminator_block)\n",
    "        discriminator_block = LeakyReLU()(discriminator_block)\n",
    "        if dropout:\n",
    "            discriminator_block = Dropout(dropout_rate)(discriminator_block)\n",
    "\n",
    "\n",
    "    discriminator_block = Flatten()(discriminator_block)\n",
    "    if concat_captions:\n",
    "        discriminator_block = Concatenate()([discriminator_block, text_attn])\n",
    "\n",
    "    dense = Dense(1, activation='sigmoid')(discriminator_block) #change to concatted if add text back\n",
    "\n",
    "    return Model(image_input, dense, name='Discriminator') #Model([text_input, image_input], dense, name='Discriminator')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan():\n",
    "    '''\n",
    "    Builds generative adversarial model for training.\n",
    "    '''\n",
    "    input_caption = Input(shape=(max_sequence_length, 100))\n",
    "    input_image = Input(shape=(128, 128, 3))\n",
    "    generated_image, generated_labels = model_text2img(input_caption)\n",
    "    discriminator_output = discriminator(generated_image) #discriminator([input_caption, generated_image])\n",
    "    \n",
    "\n",
    "    gan = Model(input_caption, [discriminator_output, generated_image, generated_labels])\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logic to train descriminator in adversarial model\n",
    "model_intermediate = build_intermediate_model((max_sequence_length, 100))\n",
    "model_text2img = build_combined_model()\n",
    "discriminator = build_discriminator(conv_channels=[4, 8, 16, 32, 64], dropout=True)\n",
    "d_adam = Adam(lr=0.00001, beta_1=0.0, beta_2=0.999)\n",
    "discriminator.compile(optimizer=d_adam, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "discriminator.trainable = False\n",
    "adversarial_net = build_gan()\n",
    "g_adam = Adam(lr=0.0001, beta_1=0.0, beta_2=0.999)\n",
    "adversarial_net.compile(optimizer=g_adam, loss=['binary_crossentropy', 'mse', 'binary_crossentropy'], loss_weights=[1, 10, 0.1], metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superclass_y = []\n",
    "for row in annot_list[:sample_count]:\n",
    "    superclass_y.append(ast.literal_eval(row[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superclass_y = np.array(superclass_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_loss_means = []\n",
    "d_acc_means = []\n",
    "a_loss_means = []\n",
    "t_loss_means = []\n",
    "classes_loss_means = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from keras.utils.generic_utils import Progbar\n",
    "from time import time\n",
    "\n",
    "\n",
    "\n",
    "# custom adversarial training logic\n",
    "BATCHSIZE = 32\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    num_data_pts = len(captions_X)\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(captions_X)\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(imgs_y)\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(caption_strings)\n",
    "\n",
    "    d_hist_loss = []\n",
    "    d_hist_acc = []\n",
    "    a_hist_loss = []\n",
    "    t_hist_loss = []\n",
    "    classes_hist_loss = []\n",
    "    print()\n",
    "    print(\"epoch {} of {}\".format(epoch+1, EPOCHS))\n",
    "    num_batches = int(num_data_pts // BATCHSIZE)\n",
    "    #print(\"number of batches: {}\".format(int(X.shape[0] // (BATCHSIZE))))\n",
    "#     progress_bar = Progbar(target=int(num_data_pts // (BATCHSIZE)))\n",
    "    minibatches_size = BATCHSIZE\n",
    "    start_time = time()\n",
    "    \n",
    "    for index in range(int(num_data_pts // (BATCHSIZE))):\n",
    "#         progress_bar.update(index)\n",
    "        \n",
    "        #print(f'{BATCHSIZE*index}:{(BATCHSIZE*(index+1))}')\n",
    "        images_real = imgs_y[BATCHSIZE*index:(BATCHSIZE*(index+1))]\n",
    "        captions_batch = captions_X[BATCHSIZE*index:(BATCHSIZE*(index+1))] + 0.1 * np.random.normal(dist_std, dist_std, (BATCHSIZE, max_sequence_length, 100))\n",
    "        labels_fake = np.zeros([BATCHSIZE,1], dtype=np.float32)\n",
    "        labels_real = np.ones([BATCHSIZE,1], dtype=np.float32)        \n",
    "        superclass_real = superclass_y[BATCHSIZE*index:(BATCHSIZE*(index+1))]\n",
    "        \n",
    "        images_fake, classes_out = model_text2img.predict(captions_batch)\n",
    "        \n",
    "\n",
    "        \n",
    "        train_imgs = np.concatenate((images_real, images_fake))\n",
    "        train_captions = np.concatenate((captions_batch, captions_batch))\n",
    "        train_labels = np.concatenate((labels_real, labels_fake))\n",
    "        \n",
    "\n",
    " \n",
    "        np.random.seed(index)\n",
    "        np.random.shuffle(train_imgs)\n",
    "        np.random.seed(index)\n",
    "        np.random.shuffle(train_captions)\n",
    "        np.random.seed(index)\n",
    "        np.random.shuffle(train_labels)\n",
    "        \n",
    "        # train discrimiator every 20th epoch\n",
    "        if index % 20 == 0:\n",
    "            d_loss, d_acc = discriminator.train_on_batch(train_imgs, train_labels)\n",
    "            d_hist_loss.append(d_loss)\n",
    "            d_hist_acc.append(d_acc)\n",
    "\n",
    "            \n",
    "        a_loss_1, _, t_loss, t_loss_class, _, t_acc, classes_acc = adversarial_net.train_on_batch(captions_batch, [labels_real, images_real, superclass_real])\n",
    "        a_hist_loss.append(a_loss_1)\n",
    "        t_hist_loss.append(t_loss)\n",
    "        classes_hist_loss.append(t_loss_class)\n",
    "        #\n",
    "# ['loss',\n",
    "#  'Discriminator_loss',\n",
    "#  'text2img_loss',\n",
    "#  'text2img_loss',\n",
    "#  'Discriminator_binary_accuracy',\n",
    "#  'text2img_binary_accuracy',\n",
    "#  'text2img_binary_accuracy_1']\n",
    "        #\n",
    "        #\n",
    "\n",
    "    classes_loss_means.append(np.mean(t_loss_class))\n",
    "    t_loss_means.append(np.mean(t_hist_loss))\n",
    "    d_loss_means.append(np.mean(d_hist_loss))\n",
    "    a_loss_means.append(np.mean(a_hist_loss))\n",
    "    d_acc_means.append(np.mean(d_hist_acc))\n",
    "    print(' ' + str(np.mean(d_hist_acc)))\n",
    "\n",
    "\n",
    "\n",
    "# adversarial_net.save_weights('.\\\\gan-v1\\\\gan-2k-epochs-weights.h5')\n",
    "# model_text2img.save_weights('.\\\\gan-v1\\\\text2img-2k-epochs-weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "def put_object(obj_file_path, obj_key):\n",
    "    try:\n",
    "        with open('/asak/aws-secret-access-key', 'r') as asak, open('/aaki/aws-access-key-id', 'r') as aaki:\n",
    "            with open (obj_file_path) as put_file:\n",
    "                session = boto3.Session(aws_secret_access_key=asak.read().strip(), aws_access_key_id = aaki.read().strip())\n",
    "                s3 = session.resource('s3')\n",
    "                client = session.client('s3', endpoint_url='https://s3.nautilus.optiputer.net')\n",
    "\n",
    "                val = client.put_object(Body=obj_file, Bucket='kevin-bucket', Key=obj_key)\n",
    "    except Exception as e:\n",
    "        print ('Error putting:')\n",
    "        print (e)\n",
    "        print ('Response:')\n",
    "        print (val)\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text2img.save_weights('/coco2017/results/text2img-prp-100epochs.h5')\n",
    "put_object('/coco2017/results/text2img-prp-100epochs.h5', 'gan-weights-100epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd_acc_means' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9b13d31bef76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Discriminator Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_acc_means\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# plt.savefig('./gan-v2/discriminator_acc-2-5k-reshaped-softmax')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd_acc_means' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.title('Discriminator Loss')\n",
    "plt.plot(d_loss_means)\n",
    "plt.savefig('/coco2017/results/discriminator_loss-prp-100epochs.png')\n",
    "plt.show()\n",
    "put_object('/coco2017/results/discriminator_loss-prp-100epochs.png', 'd_loss-100epochs')\n",
    "\n",
    "\n",
    "plt.title('Discriminator Accuracy')\n",
    "plt.plot(d_acc_means)\n",
    "plt.savefig('/coco2017/results/discriminator_acc-prp-100epochs.png')\n",
    "plt.show()\n",
    "put_object('/coco2017/results/discriminator_acc-prp-100epochs.png', 'd_acc-100epochs')\n",
    "\n",
    "plt.title('Adversarial Loss')\n",
    "plt.plot(a_loss_means)\n",
    "plt.savefig('/coco2017/results/adversarial_loss-prp-100epochs.png')\n",
    "plt.show()\n",
    "put_object('/coco2017/results/adversarial_loss-prp-100epochs.png', 'adv_loss-100epochs')\n",
    "\n",
    "plt.title('Text To Image Loss')\n",
    "plt.plot(t_loss_means)\n",
    "plt.savefig('/coco2017/results/text2img_loss-prp-100epochs.png')\n",
    "plt.show()\n",
    "put_object('/coco2017/results/text2img_loss-prp-100epochs.png', 't2i_loss-100epochs')\n",
    "\n",
    "plt.title('Classes Loss')\n",
    "plt.plot(classes_loss_means)\n",
    "plt.savefig('/coco2017/results/classes_loss-prp-100epochs.png')\n",
    "plt.show()\n",
    "put_object('/coco2017/results/classes_loss-prp-100epochs.png', 'classes_loss-100epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
